{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read Mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lnas import LnasFormat\n",
    "import pathlib\n",
    "\n",
    "meshes_path = pathlib.Path(\"../insight/Docker/local/volume/PrologisCajamar4_full/G100/mesh\")\n",
    "\n",
    "mesh = LnasFormat.from_file(meshes_path / \"G100.lnas\")\n",
    "negative_mesh = LnasFormat.from_file(meshes_path / \"G100_negative.lnas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read historic series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13995/3235872955.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pathlib\n",
    "\n",
    "data_path = pathlib.Path(\"../insight/Docker/local/volume/PrologisCajamar4_full/G100/cases/013/data/body_data\")\n",
    "\n",
    "body_hist_series = pd.read_hdf(data_path / \"bodies.G100_hs.data.h5\")\n",
    "negative_body_hist_series = pd.read_hdf(data_path / \"bodies.G100_neg_hs.data.h5\")\n",
    "\n",
    "last_index = body_hist_series.point_idx.max()\n",
    "negative_body_hist_series.point_idx += last_index + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Offset body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "NORMAL_OFFSET = - 0.125\n",
    "\n",
    "\n",
    "for surface_name in negative_mesh.surfaces.keys():\n",
    "    tri_normals = negative_mesh.geometry_from_surface(surface_name).normals.copy()\n",
    "    tri_verts = negative_mesh.geometry_from_surface(surface_name).triangle_vertices.copy()\n",
    "    tri_verts += np.repeat(tri_normals, 3, axis=0).reshape(-1, 3, 3) * NORMAL_OFFSET\n",
    "\n",
    "    new_verts = tri_verts.copy().reshape(-1, 3)\n",
    "    new_verts = np.unique(new_verts, axis=0)\n",
    "\n",
    "    new_triangles = []\n",
    "\n",
    "    for tri in tri_verts:\n",
    "        tri_index = []\n",
    "        for v in tri:\n",
    "            matches = np.all(new_verts == v, axis=1)\n",
    "            indices = np.where(matches)\n",
    "            tri_index.append(indices[0][0])\n",
    "        tri_index.reverse()\n",
    "        new_triangles.append(np.array(tri_index))\n",
    "\n",
    "    new_triangles = np.array(new_triangles, dtype=np.uint32) + len(mesh.geometry.vertices)\n",
    "\n",
    "    new_tri_indices = np.arange(\n",
    "        mesh.geometry.triangles.shape[0], mesh.geometry.triangles.shape[0] + new_triangles.shape[0]\n",
    "    )\n",
    "    mesh.surfaces[surface_name + \"_inside\"] = new_tri_indices\n",
    "\n",
    "    mesh.geometry.vertices = np.concatenate((mesh.geometry.vertices, new_verts))\n",
    "    mesh.geometry.triangles = np.concatenate((mesh.geometry.triangles, new_triangles))\n",
    "\n",
    "    mesh.geometry._update_triangles_vertices()\n",
    "    mesh.geometry._update_normals()\n",
    "    mesh.geometry._update_areas()\n",
    "    mesh.geometry._update_vertices_normals()\n",
    "\n",
    "\n",
    "filename = meshes_path / \"G100.merged.lnas\"\n",
    "export_path = meshes_path / \"G100.merged.stl\"\n",
    "\n",
    "mesh.to_file(filename)\n",
    "mesh.export_stl(export_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join historic series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mO Kernel falhou ao executar o código na célula atual ou em uma célula anterior. Examine o código nas células para identificar uma possível causa da falha. Clique <a href=\"https://aka.ms/vscodeJupyterKernelCrash\">aqui</a> para obter mais informações. Consulte o <a href='command:jupyter.viewOutput'>log</a> do Jupyter para obter mais detalhes."
     ]
    }
   ],
   "source": [
    "new_df = pd.concat([body_hist_series, negative_body_hist_series]).sort_values(\n",
    "    by=[\"time_step\", \"point_idx\"]\n",
    ")\n",
    "new_df.to_hdf(path_or_buf=data_path / \"bodies.G100.merged.data.h5\", key=\"df\", mode=\"w\")\n",
    "\n",
    "new_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cfdmod-XMkUSlb0-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
